# 04 Streaming: リアルタイム応答チャットボット 🌊💬

## はじめに 🌟
この実装ステップでは、03_RAGで作成した外部知識を活用するチャットボットに、
ストリーミング応答機能を追加します。これにより、ユーザーはAIの思考プロセスを
リアルタイムで観察でき、より自然な対話体験を得ることができます。

## システムの動作フロー 🏄‍♂️
1. ユーザーがフロントエンドでメッセージを入力
2. フロントエンドが`session_id`とともにバックエンドAPIにリクエストを送信
3. バックエンドがリクエストを受信し、ベクトルストアから関連情報を検索
4. 検索結果と会話履歴を考慮してAIモデルが応答を生成し、チャンク単位で返信
5. フロントエンドが受信したチャンクをリアルタイムでユーザーに表示
6. 応答生成が完了すると、フロントエンドが最終的な応答を表示

## 開発ステップ 🏗️

### 1. バックエンド拡張
FastAPIで構築したチャットボットのAPIサーバーにストリーミング機能を追加します。  
LangChainの非同期ストリーミング機能を利用して、応答をチャンク単位で返信します。  
詳細な手順は[バックエンドの実装ガイド](backend/README.md)を参照してください。

### 2. フロントエンド調整
Chainlitで構築したチャットインターフェースを更新し、ストリーミング応答を
リアルタイムで表示できるようにします。  
詳細な手順は[フロントエンドの実装ガイド](frontend/README.md)を参照してください。

## 主な変更点 📝

### バックエンド（chat_server.py）:
1. 新規モジュールのインポート:
   - `StreamingResponse`をFastAPIからインポート

2. ストリーミング応答関数の実装:
   - `stream_chat_response`関数を追加し、非同期ジェネレータとして実装

3. チャットエンドポイントの更新:
   - `StreamingResponse`を使用してストリーミング応答を返すように変更

### フロントエンド（frontend_app.py）:
1. HTTPクライアントの変更:
   - `requests`から非同期クライアント`httpx`に変更

2. メッセージ処理の更新:
   - 空の応答メッセージを作成し、チャンク単位で更新するように変更
   - `stream_token`メソッドを使用して、受信したチャンクをリアルタイムで表示

3. ストリーミング処理の実装:
   - 非同期コンテキストマネージャを使用して、ストリーミング応答を処理
